---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a second-year M.S. student at SKKU (Sungkyunkwan University), South Korea, advised by <a href='https://www.csehong.com/'>Prof. Sungeun Hong</a> in the Artificial Intelligence and Media Lab (<a href='https://aim.skku.edu/home'>AIM Lab</a>). My research focuses on building robust and adaptive AI through multimodal learning and test-time adaptation. I have proposed novel frameworks for semantic segmentation and classification under domain shifts, leveraging class-wise alignment and adaptive thresholding. Recently, I introduced a question-aware temporal reasoning model for audio-visual question answering, which integrates Gaussian experts and modality interaction. My recent work expands into audio-visual reasoning and zero-shot test-time adaptation via prompt-based modality fusion.

<!-- My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a>  -->
<!-- (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->


# ğŸ”¥ News
- *2025.04*: &nbsp;ğŸ‰ğŸ‰ Question-Aware Gaussian Experts for Audio-Visual Question Answering was selected as highlight at CVPR 2025!
- *2025.02*: &nbsp;ğŸ‰ğŸ‰ Question-Aware Gaussian Experts for Audio-Visual Question Answering is accpeted at CVPR 2025!
- *2025.02*: &nbsp;ğŸ‰ğŸ‰ Winner of the Honorable Mention for Outstanding Paper Award at IPIU 2025.
- *2024.10*: &nbsp;ğŸ‰ğŸ‰ Prototypical Class-wise Test-Time Adaptation is accepted at Pattern Recognition Letters!

# ğŸ“ Publications 
<span style="font-size: smaller;">Equal contribution are denoted by *</span>
<!-- <div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/qa-tiger.png' alt="sym" width="100%"></div></div> -->
<div class='paper-box-text' markdown="1">

- <strong>Question-Aware Gaussian Experts for Audio-Visual Question Answering, <span style="color:blue">CVPR 2025</span> <span style="color:red">Highlight</span></strong> <span style="font-size: smaller;">(Accept. rate 22.12%)</span><br> Hongyeob Kim*, <span style="color:blue"><strong>Inyoung Jung</strong></span>*, Dayoon Suh, Youjia Zhang, Sangmin Lee, and Sungeun Hong<br>[[project page]](https://aim-skku.github.io/QA-TIGER/) [[arXiv]](https://arxiv.org/abs/2503.04459) [[code]](https://github.com/AIM-SKKU/QA-TIGER) <a href='https://scholar.google.com/citations?view_op=view_citation&hl=ko&user=eiissBIAAAAJ&citation_for_view=eiissBIAAAAJ:d1gkVwhDpl0C'><img src="https://img.shields.io/badge/Google%20Scholar-View-blue?logo=Google%20Scholar&style=flat"></a>
<!-- <img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a> -->

- <strong>Prototypical Class-wise Test-Time Adaptation, <span style="color:blue">Pattern Recognition Letters 2025</span></strong><br>Hojoon Lee, Seunghwan Lee, **<span style="color:blue">Inyoung Jung</span>**, and Sungeun Hong<br>[[DOI]](https://www.google.com/url?q=https%3A%2F%2Fdoi.org%2F10.1016%2Fj.patrec.2024.10.011&sa=D&sntz=1&usg=AOvVaw2V6yZknKj_BL1GeClW6opZ) [[paper]](https://www.sciencedirect.com/science/article/pii/S016786552400299X/pdfft?casa_token=PcnO8FCxc_UAAAAA:R31sNDSXlvyo9gwRaVIUl_r5zqh9oTPoSHg8iepo8G0StOxWF_7q9CP10ZdzJ1oGGC5SZx4fXXI&md5=e83a2446f29e2b60b374c913fb291e36&pid=1-s2.0-S016786552400299X-main.pdf) <a href="https://scholar.google.com/citations?view_op=view_citation&hl=ko&user=eiissBIAAAAJ&citation_for_view=eiissBIAAAAJ:d1gkVwhDpl0C"><img src="https://img.shields.io/badge/Google%20Scholar-View-blue?logo=Google%20Scholar&style=flat"></a>


</div>

# ğŸ’¡ Projects
- *2024.09 - 2025.08*: <strong>Zero-Shot Question-Answering Framework with Adaptive Audio-Visual Fusion Prompts</strong>, *M.S. Students Fellowship by National Research Foundation of Korea (NRF)*
- *2024.07 - 2024.12*: <strong>Robust Deep Semantic Segmentation Model for Continual Domain Shifts</strong>, *Capstone Design in Information and Communication Engineering by Inha University*

# ğŸ– Honors and Awards
- *2025.02*: <strong>Honorable Mention for Outstanding Paper Award</strong>, IPIU 2025.
- *2024.12*: <strong>Excellence Award in Capstone Design for Information and Communication Engineering</strong>, Inha University.

# ğŸ“– Educations
- *2024.03 - present*: Sungkyunkwan University, M.S. Student, Department of Immersive Media Engineering.
- *2021.03 - 2024.02*: Inha University, Bachelor or Engineering, Department of Information and Communication Engineering.

# ğŸ’» Internships
- *2025.01 - 2025.02*: AI Research Engineer Intern @ [ARI.ai](https://www.ariai.io/).
- *2022.12 - 2024.02*: Artificial Intelligence and Media Lab @ [Sungkyunkwan Univ](https://aim.skku.edu/).